{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates a simple function-calling feature of a large language model (LLM) using a local model. We use [Ollama](https://github.com/ollama/ollama) to serve the local model. Additionally, we utilize LangChain's Tools calling feature, which is another term for function calling.\n",
    "\n",
    "The example generates function-calling information based on the incoming question, similar to OpenAI's function-calling feature described [here](https://platform.openai.com/docs/guides/function-calling)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement\n",
    "Let's assume we are developing a customer service chatbot that takes in a userâ€™s phone number and checks if:\n",
    "- The phone number is inactive and needs reactivation.\n",
    "- It accepts the request to reactivate the phone number.\n",
    "\n",
    "This is a simple, one-step use case for the agent, which could be part of a larger agentic pattern handling more complex use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.render import render_text_description\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Custom Tools/ Functions\n",
    "Read about tools calling in langchain [here](https://blog.langchain.dev/tool-calling-with-langchain/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def phone_status_checker(phone: str):\n",
    "    \"\"\"Useful when you want to check the status of a phone number\"\"\"\n",
    "    phone_statuses = {\"1234567890\": True, \"0987654321\": False}\n",
    "    status = phone_statuses.get(phone, False)\n",
    "    return {\"active\": status}\n",
    "\n",
    "\n",
    "@tool\n",
    "def phone_reactivator(phone: str):\n",
    "    \"\"\"Useful when you want to reactivate a phone number\"\"\"\n",
    "    return {\"message\": \"Your request for phone reactivation is submitted. Thanks\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define main class\n",
    "The entire workflow is defined using the Langchain Expression Language [LCEL](https://python.langchain.com/v0.1/docs/expression_language/) and [Runnables](https://python.langchain.com/v0.1/docs/expression_language/interface/) pipe syntax, which greatly enhances code readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhoneStatusAgent:\n",
    "    def __init__(self):\n",
    "        self.llm = Ollama(model=\"llama3\", format=\"json\")\n",
    "        self.tools = [phone_status_checker, phone_reactivator]\n",
    "        rendered_tools = render_text_description(self.tools)\n",
    "        system_prompt = f\"\"\"You are an assistant that has access to the following set of tools.\n",
    "            Here are the names and descriptions for each tool:\n",
    "\n",
    "            {rendered_tools}\n",
    "            Given the user input, return the name and input of the tool to use.\n",
    "            Return your response as a JSON blob with 'name' and 'arguments' keys.\n",
    "            The value associated with the 'arguments' key should be a dictionary of parameters.\"\"\"\n",
    "        self.prompt = ChatPromptTemplate.from_messages(\n",
    "            [(\"system\", system_prompt), (\"user\", \"{input}\")]\n",
    "        )\n",
    "        self.chat_history = []\n",
    "\n",
    "    def run(self, input_text):\n",
    "        question = input_text\n",
    "\n",
    "\t\t\t\t# Here first three chain components generate the correct function calling information.\n",
    "\t\t\t\t# While the final chain component takes the function_name and arguments information and invokes the actual function.\n",
    "        chain = self.prompt | self.llm | JsonOutputParser() | self.tool_chain\n",
    "        response = chain.invoke({\"input\": question})\n",
    "        return response\n",
    "\n",
    "    def tool_chain(self, model_output):\n",
    "        tool_map = {tool.name: tool for tool in self.tools}\n",
    "        chosen_tool = tool_map[model_output[\"name\"]]\n",
    "        return itemgetter(\"arguments\") | chosen_tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execute the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = PhoneStatusAgent()\n",
    "print(pp.run('Submit phone reactivate request: 0987654321'))\n",
    "print(pp.run('Please reactivate my phone number: 1234567890'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More on Langchain\n",
    "LangChain has developed multiple interfaces _(`create_tool_calling_agent`)_ to support the function-calling feature for models that support it. See more details [here](https://python.langchain.com/v0.1/docs/modules/agents/agent_types/tool_calling/)\n",
    "\n",
    "- Rigorous testing and research is required to finalize the model for function calling when integrated with LangChain's interfaces.\n",
    "- Look for models which natively support it like OpenAI's chatgpt4.0, mixtral. They are known to perform reasonably well for this feature."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
