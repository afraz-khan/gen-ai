{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3263d842-1249-44dd-82d4-820d0e91cc31",
   "metadata": {},
   "source": [
    "# Intro\n",
    "This Notebook describes step by step procedure to perform a simple RAG based task.\n",
    "For knowledge base, it uses a PDF document present at your local directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac61f5d7-5646-401a-9b71-2d6ccde1b035",
   "metadata": {},
   "source": [
    "### 1. Document Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1cb036-90fc-4dcd-a0d8-a6d0d601d0cd",
   "metadata": {},
   "source": [
    "Install packages if not installed already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e11216-36b7-4eb7-b2c5-13a6ba864b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pypdf tiktoken python-dotenv openai psycopg2-binary pgvector langchain_postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ca96aa-e821-4a1b-ac06-aa7a0038e792",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"/path/document.pdf\") # provide local path to the pdf doc\n",
    "pages = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d910105-6cdb-4973-b803-d491d497fc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pages[3].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b40b1d6-3b13-426e-abeb-40c7c48786ca",
   "metadata": {},
   "source": [
    "### 2. Documents Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35460d1-2db6-47bc-b044-bfc58f6ccb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "text_splitter = TokenTextSplitter(chunk_size=1500, chunk_overlap=150)\n",
    "docs = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8563d19a-6047-495e-9250-05ce50677fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c375f7bc-ec12-44dc-a392-9d837f76f996",
   "metadata": {},
   "source": [
    "### 3. Create Embeddings\n",
    "\n",
    "We are using Ollama's embeddings with same main model.\n",
    "`nomic-embed-text` is mentioned on langchain for Ollama embeddings but we keep using the same LLM(`llama3`) for embedding as well, it's your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5f3219-ab87-4726-853a-909ff3573711",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "embeddings = OllamaEmbeddings(model='llama3')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69aa468-753b-44bf-8357-3cdfadd95785",
   "metadata": {},
   "source": [
    "#### > Optional to view the embeddings metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cd63af-c14e-45ea-bbe2-40861e2ef14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_vectors = embeddings.embed_documents([d.page_content for d in docs]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f209eb7c-c831-49f1-b982-3e66742364be",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_vectors[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0038ae-95c6-4d45-84fe-fcfe7520977a",
   "metadata": {},
   "source": [
    "### 4. PGVector Database \n",
    "\n",
    "Make sure you have a pgvector database installed in your machine.\n",
    "\n",
    "See ==> **[here](https://github.com/pgvector/pgvector?tab=readme-ov-file#docker)** for installation via docker.\n",
    "\n",
    "Run the below command to run the pgvector database container: </br>\n",
    "`docker run --name {container-name} -e POSTGRES_PASSWORD -p 5432:5432 pgvector/pgvector:pg16`\n",
    "> Make sure, you dont have any other postgres server running at the port 5432.\n",
    "\n",
    "Open the vector db server in some db manager and create a new database named `vector_db`\n",
    "\n",
    "Create the vector extesion for the `vector_db` db by running: </br>\n",
    "`CREATE EXTENSION vector`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9228b99-89ec-43d5-9472-b805fff92571",
   "metadata": {},
   "source": [
    "Update the below code as per your pg settings and run it to save the vector embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbc4621-dceb-40cc-8f6d-f23db9ba432d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_postgres.vectorstores import PGVector\n",
    "\n",
    "connection = \"postgresql+psycopg://{username}:{password}@localhost:5432/vector_db\" # connection string\n",
    "collection_name = \"my_docs\"\n",
    "\n",
    "vectorstore = PGVector(\n",
    "    embeddings=embeddings,\n",
    "    collection_name=collection_name,\n",
    "    connection=connection,\n",
    "    use_jsonb=True,\n",
    ")\n",
    "\n",
    "vectorstore.add_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d573179-f072-4220-a4bd-74b71f6b9f24",
   "metadata": {},
   "source": [
    "### 5. Perform similarity search\n",
    "\n",
    "Put your question/query in the `query` variable below that you want to send to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9107bf-a883-415b-9d06-372b23ddf55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"Tell me how Environmental Control System works?\"\n",
    "# query = \"When was the first Flight Data Recorder (FDR) was employed?\"\n",
    "# query = \"What sub-systems are integrated in the Environmental Control System(ECS)?\"\n",
    "\n",
    "similars = vectorstore.similarity_search_with_score(query, k=5)\n",
    "\n",
    "for doc in similars:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df860a6-f39f-4064-88a6-6e2c7f7ad65a",
   "metadata": {},
   "source": [
    "### 6. Output with base **LLMChain** class\n",
    "\n",
    "We use the LLMChain chain to perform the simple Q/A-based tasks, nothing fancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26c8e6a-e5bc-4c3a-8b02-46b3692b272e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "llm = Ollama(\n",
    "    model=\"llama3\"\n",
    ")  # assuming you have Ollama installed and have llama3 model pulled with `ollama pull llama3\n",
    "\n",
    "template = \"\"\"You are an aircraft expert who answers the questions to the best of its knowledge. If you don't know the answer,\n",
    " just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible.\n",
    " Always say \"thanks for asking!\" at the end of the answer. \n",
    "\n",
    "-----------\n",
    "Centext:\n",
    "\n",
    "{context}\n",
    "\n",
    "------------\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# print(similars[0][0].page_content)\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt, verbose=True)\n",
    "response = chain.run({\"question\": query, \"context\": \"\\n\\n\".join([d[0].page_content for d in similars])})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fb6615-c946-4924-b3cd-c2a95854afc5",
   "metadata": {},
   "source": [
    "#### Format your results for better readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c56ccfe-74aa-4a9a-9d88-c0668506da05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown, Latex\n",
    "\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c18664-424b-4ab2-b025-5bc22896f9a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen_ai_kernel",
   "language": "python",
   "name": "gen_ai_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
