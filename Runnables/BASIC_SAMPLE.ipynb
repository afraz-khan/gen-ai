{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Chain Task\n",
    "See below a function calling example which generates the output in a single chain. see the `run` method. It has four components/[runnables](https://python.langchain.com/v0.1/docs/expression_language/interface/) all in one chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhoneStatusAgent:\n",
    "    def __init__(self):\n",
    "        self.llm = Ollama(model=\"llama3\", format=\"json\")\n",
    "        self.tools = [phone_status_checker, phone_reactivator]\n",
    "        rendered_tools = render_text_description(self.tools)\n",
    "        system_prompt = f\"\"\"You are an assistant that has access to the following set of tools.\n",
    "            Here are the names and descriptions for each tool:\n",
    "\n",
    "            {rendered_tools}\n",
    "            Given the user input, return the name and input of the tool to use.\n",
    "            Return your response as a JSON blob with 'name' and 'arguments' keys.\n",
    "            The value associated with the 'arguments' key should be a dictionary of parameters.\"\"\"\n",
    "        self.prompt = ChatPromptTemplate.from_messages(\n",
    "            [(\"system\", system_prompt), (\"user\", \"{input}\")]\n",
    "        )\n",
    "        self.chat_history = []\n",
    "\n",
    "    def run(self, input_text):\n",
    "        question = input_text\n",
    "        chain = self.prompt | self.llm | JsonOutputParser() | self.tool_chain\n",
    "        response = chain.invoke({\"input\": question})\n",
    "        return response\n",
    "\n",
    "    def tool_chain(self, model_output):\n",
    "        tool_map = {tool.name: tool for tool in self.tools}\n",
    "        chosen_tool = tool_map[model_output[\"name\"]]\n",
    "        return itemgetter(\"arguments\") | chosen_tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runnable/Chain Components\n",
    "The main chain in the above example consists of four components:\n",
    "\n",
    "`self.prompt | self.llm | JsonOutputParser() | self.tool_chain`\n",
    "\n",
    "However, you can modify it according to your use case. For instance, if you need the output of `JsonOutputParser()` for something else before running the final chain `self.tool_chain`, you can split the chain into two parts: one with the first three components and the other with the last one.\n",
    "\n",
    "Letâ€™s say we want to produce the final answer using the LLM, and for that, we need the generated function-calling information _(output of `JsonOutputParser()`)_ to generate the correct response. We can update our main PhoneStatusAgent class as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhoneNumberStatusAgent:\n",
    "    def __init__(self):\n",
    "        self.llm = Ollama(model=\"llama3\", format=\"json\")\n",
    "        self.tools = [phone_number_status_checker, phone_number_reactivator]\n",
    "        rendered_tools = render_text_description(self.tools)\n",
    "\n",
    "        tool_system_prompt = f\"\"\"You are an assistant that has access to the following set of tools.\n",
    "            Here are the names and descriptions for each tool:\n",
    "\n",
    "            {rendered_tools}\n",
    "            Given the user input, return the name and input of the tool to use.\n",
    "            Return your response as a JSON blob with 'name' and 'arguments' keys.\n",
    "            The value associated with the 'arguments' key should be a dictionary of parameters.\"\"\"\n",
    "        self.tool_prompt = ChatPromptTemplate.from_messages(\n",
    "            [(\"system\", tool_system_prompt), (\"user\", \"{input}\")]\n",
    "        )\n",
    "\n",
    "        response_template = \"\"\"You are a helpful assistant who generates a generic message based on the output of a tool execution.\n",
    "\n",
    "        Tool Output Information:\n",
    "        {tool_execution}\n",
    "\n",
    "        Response Instructions:\n",
    "        - If phone number status is inactive, ask, \"Do you want to reactivate the phone number?\"\n",
    "        - Remove any information related to the tool.\n",
    "        - Return your response as a JSON object with a 'message' key.\n",
    "        \"\"\"\n",
    "        self.response_prompt = ChatPromptTemplate.from_template(response_template)\n",
    "\n",
    "    def run(self, input_text):\n",
    "        question = input_text\n",
    "\n",
    "\t\t\t\t# Chain one with three components\n",
    "        tool_chain = self.tool_prompt | self.llm | JsonOutputParser()\n",
    "        tool_chain_output = tool_chain.invoke({\"input\": question})\n",
    "\n",
    "        tool_name = tool_chain_output[\"name\"]\n",
    "        tool_arguments = tool_chain_output[\"arguments\"]\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Chain two with only one component which is basichally to invoke the actual tool/function\n",
    "        tool_output = self.get_tool(tool_name).invoke({\"input\": \"\", **tool_arguments})\n",
    "\n",
    "\t\t\t\t# Chain three to generate the final response\n",
    "        response_chain = self.response_prompt | self.llm | JsonOutputParser()\n",
    "        response = response_chain.invoke(\n",
    "            {\"tool_execution\": {\"tool_name\": tool_name, \"tool_output\": tool_output}}\n",
    "        )\n",
    "\n",
    "        return response\n",
    "\n",
    "    def get_tool(self, tool_name):\n",
    "        tool_map = {tool.name: tool for tool in self.tools}\n",
    "        chosen_tool = tool_map[tool_name]\n",
    "        return chosen_tool"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
